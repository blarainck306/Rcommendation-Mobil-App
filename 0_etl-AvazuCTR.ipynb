{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"0_etl-AvazuCTR.ipynb","provenance":[{"file_id":"1pwejneaZX5yQs-fWXNonEKXg0mOQoeEC","timestamp":1609861702094}],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyPOk+zJUr6VhroSUavIs/o0"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WKmCd9xMi2F8","executionInfo":{"status":"ok","timestamp":1610054705559,"user_tz":300,"elapsed":665,"user":{"displayName":"Yu Liu","photoUrl":"","userId":"08196854291231106021"}},"outputId":"c9b98069-8cc3-4424-807a-cf19d6be9299"},"source":["first_time = True\n","!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zSsSCyZojykN","executionInfo":{"status":"ok","timestamp":1610054731083,"user_tz":300,"elapsed":24623,"user":{"displayName":"Yu Liu","photoUrl":"","userId":"08196854291231106021"}},"outputId":"45a288b1-84b2-4a42-9e2b-af2c06a0ef8f"},"source":["if first_time:\n","  from google.colab import drive\n","  drive.mount(\"/content/drive\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HJG0iAsij0cF","executionInfo":{"status":"ok","timestamp":1610054779524,"user_tz":300,"elapsed":59449,"user":{"displayName":"Yu Liu","photoUrl":"","userId":"08196854291231106021"}},"outputId":"db428cd5-30b3-4065-cd87-2326882e3e34"},"source":["if first_time:\n","  !apt-get update\n","  !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","  !wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz\n","  !tar xf spark-3.0.0-bin-hadoop2.7.tgz"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n","Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n","Get:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]\n","Get:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [833 B]\n","Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Get:8 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Get:9 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [42.7 kB]\n","Get:10 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Get:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Packages [66.5 kB]\n","Hit:14 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Get:15 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Hit:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,376 kB]\n","Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,707 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [53.3 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,274 kB]\n","Get:21 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,846 kB]\n","Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [277 kB]\n","Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,140 kB]\n","Get:24 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [874 kB]\n","Fetched 10.9 MB in 5s (2,057 kB/s)\n","Reading package lists... Done\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d9K3FbcDj23e","executionInfo":{"status":"ok","timestamp":1610054804585,"user_tz":300,"elapsed":25049,"user":{"displayName":"Yu Liu","photoUrl":"","userId":"08196854291231106021"}},"outputId":"ca4ba5d9-f5be-47e7-9aa4-41c3a14c9df0"},"source":["# Set up Spark\n","!pip install -q findspark\n","!pip install py4j\n","\n","!export JAVA_HOME=$(/usr/lib/jvm/java-8-openjdk-amd64 -v 1.8)\n","! echo $JAVA_HOME\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop2.7\"\n","import findspark\n","findspark.init(\"spark-3.0.0-bin-hadoop2.7\")# SPARK_HOME\n","\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n","\n","if first_time:\n","  !pip install koalas\n","  !pip install --upgrade PyArrow\n","\n","import time\n","import databricks.koalas as ks\n","import numpy as np\n","import pandas as pd\n","from IPython.display import display # for displaying multiple  dataframes from a single cell\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import math\n","from numpy.linalg import norm\n","import os\n","os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n","\n","import os\n","os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n","\n","%matplotlib inline\n","from pyspark.sql import SparkSession\n","spark = SparkSession \\\n","    .builder \\\n","    .appName(\"AvazuCTR\") \\\n","    .config(\"spark.some.config.option\", \"some-value\") \\\n","    .getOrCreate()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting py4j\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/42/25ad191f311fcdb38b750d49de167abd535e37a144e730a80d7c439d1751/py4j-0.10.9.1-py2.py3-none-any.whl (198kB)\n","\r\u001b[K     |█▋                              | 10kB 19.3MB/s eta 0:00:01\r\u001b[K     |███▎                            | 20kB 11.9MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 40kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 51kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 61kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 71kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 81kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 92kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 102kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 112kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 122kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 133kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 143kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 153kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 163kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 174kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 184kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 194kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 5.7MB/s \n","\u001b[?25hInstalling collected packages: py4j\n","Successfully installed py4j-0.10.9.1\n","/bin/bash: /usr/lib/jvm/java-8-openjdk-amd64: Is a directory\n","\n","Collecting koalas\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/ab/cbc150384b90ad1dd63d5371a32be80196aed461b53b974b60e087ec7cb6/koalas-1.5.0-py3-none-any.whl (792kB)\n","\u001b[K     |████████████████████████████████| 798kB 5.8MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow>=0.10 in /usr/local/lib/python3.6/dist-packages (from koalas) (0.14.1)\n","Requirement already satisfied: matplotlib<3.3.0,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from koalas) (3.2.2)\n","Requirement already satisfied: pandas>=0.23.2 in /usr/local/lib/python3.6/dist-packages (from koalas) (1.1.5)\n","Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from koalas) (1.19.4)\n","Requirement already satisfied: six>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pyarrow>=0.10->koalas) (1.15.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib<3.3.0,>=3.0.0->koalas) (0.10.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<3.3.0,>=3.0.0->koalas) (2.8.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<3.3.0,>=3.0.0->koalas) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<3.3.0,>=3.0.0->koalas) (1.3.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.2->koalas) (2018.9)\n","Installing collected packages: koalas\n","Successfully installed koalas-1.5.0\n","Collecting PyArrow\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7MB)\n","\u001b[K     |████████████████████████████████| 17.7MB 467kB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from PyArrow) (1.19.4)\n","Installing collected packages: PyArrow\n","  Found existing installation: pyarrow 0.14.1\n","    Uninstalling pyarrow-0.14.1:\n","      Successfully uninstalled pyarrow-0.14.1\n","Successfully installed PyArrow-2.0.0\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"MOQXno8Mqs0f"},"source":["## load data"]},{"cell_type":"code","metadata":{"id":"akAVe57UwZre"},"source":["!cp /content/drive/MyDrive/LAI_OFFER/BIG_DATA/AvazuCTR/Data/ARCHIVED/train.csv ."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IWCZB4M0jk4M"},"source":["df = spark.read.csv(\"train.csv\",inferSchema=True,header=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lO5zmYjxkSYX","executionInfo":{"status":"ok","timestamp":1610056284706,"user_tz":300,"elapsed":81787,"user":{"displayName":"Yu Liu","photoUrl":"","userId":"08196854291231106021"}},"outputId":"eac05f43-3079-43f2-c7c1-7068faad178a"},"source":["df.show(10)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+--------------------+-----+--------+----+----------+--------+-----------+-------------+--------+----------+------------+---------+---------+------------+-----------+----------------+-----+---+---+----+---+---+------+---+\n","|                  id|click|    hour|  C1|banner_pos| site_id|site_domain|site_category|  app_id|app_domain|app_category|device_id|device_ip|device_model|device_type|device_conn_type|  C14|C15|C16| C17|C18|C19|   C20|C21|\n","+--------------------+-----+--------+----+----------+--------+-----------+-------------+--------+----------+------------+---------+---------+------------+-----------+----------------+-----+---+---+----+---+---+------+---+\n","| 1000009418151094273|    0|14102100|1005|         0|1fbe01fe|   f3845767|     28905ebd|ecad2386|  7801e8d9|    07d7df22| a99f214a| ddd2926e|    44956a24|          1|               2|15706|320| 50|1722|  0| 35|    -1| 79|\n","|10000169349117863715|    0|14102100|1005|         0|1fbe01fe|   f3845767|     28905ebd|ecad2386|  7801e8d9|    07d7df22| a99f214a| 96809ac8|    711ee120|          1|               0|15704|320| 50|1722|  0| 35|100084| 79|\n","|10000371904215119486|    0|14102100|1005|         0|1fbe01fe|   f3845767|     28905ebd|ecad2386|  7801e8d9|    07d7df22| a99f214a| b3cf8def|    8a4875bd|          1|               0|15704|320| 50|1722|  0| 35|100084| 79|\n","|10000640724480838376|    0|14102100|1005|         0|1fbe01fe|   f3845767|     28905ebd|ecad2386|  7801e8d9|    07d7df22| a99f214a| e8275b8f|    6332421a|          1|               0|15706|320| 50|1722|  0| 35|100084| 79|\n","|10000679056417042096|    0|14102100|1005|         1|fe8cc448|   9166c161|     0569f928|ecad2386|  7801e8d9|    07d7df22| a99f214a| 9644d0bf|    779d90c2|          1|               0|18993|320| 50|2161|  0| 35|    -1|157|\n","|10000720757801103869|    0|14102100|1005|         0|d6137915|   bb1ef334|     f028772b|ecad2386|  7801e8d9|    07d7df22| a99f214a| 05241af0|    8a4875bd|          1|               0|16920|320| 50|1899|  0|431|100077|117|\n","|10000724729988544911|    0|14102100|1005|         0|8fda644b|   25d4cfcd|     f028772b|ecad2386|  7801e8d9|    07d7df22| a99f214a| b264c159|    be6db1d7|          1|               0|20362|320| 50|2333|  0| 39|    -1|157|\n","|10000918755742328737|    0|14102100|1005|         1|e151e245|   7e091613|     f028772b|ecad2386|  7801e8d9|    07d7df22| a99f214a| e6f67278|    be74e6fe|          1|               0|20632|320| 50|2374|  3| 39|    -1| 23|\n","|10000949271186029916|    1|14102100|1005|         0|1fbe01fe|   f3845767|     28905ebd|ecad2386|  7801e8d9|    07d7df22| a99f214a| 37e8da74|    5db079b5|          1|               2|15707|320| 50|1722|  0| 35|    -1| 79|\n","|10001264480619467364|    0|14102100|1002|         0|84c7ba46|   c4e18dd6|     50e219e0|ecad2386|  7801e8d9|    07d7df22| c357dbff| f1ac7184|    373ecbe6|          0|               0|21689|320| 50|2496|  3|167|100191| 23|\n","+--------------------+-----+--------+----+----------+--------+-----------+-------------+--------+----------+------------+---------+---------+------------+-----------+----------------+-----+---+---+----+---+---+------+---+\n","only showing top 10 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eXRTWF1cqvEI"},"source":["## count"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j_3C0_EJkWq2","executionInfo":{"status":"ok","timestamp":1610056296891,"user_tz":300,"elapsed":91246,"user":{"displayName":"Yu Liu","photoUrl":"","userId":"08196854291231106021"}},"outputId":"1d0ebb08-beed-4ff2-ee01-9dbe475407f5"},"source":["df.count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["40428967"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"L93aNU57qwVI"},"source":["## Schema"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hHYGoUibkb3E","executionInfo":{"status":"ok","timestamp":1610056296892,"user_tz":300,"elapsed":89294,"user":{"displayName":"Yu Liu","photoUrl":"","userId":"08196854291231106021"}},"outputId":"2b589842-9b71-44fd-c96a-943ed5608db0"},"source":["df.printSchema()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["root\n"," |-- id: decimal(20,0) (nullable = true)\n"," |-- click: integer (nullable = true)\n"," |-- hour: integer (nullable = true)\n"," |-- C1: integer (nullable = true)\n"," |-- banner_pos: integer (nullable = true)\n"," |-- site_id: string (nullable = true)\n"," |-- site_domain: string (nullable = true)\n"," |-- site_category: string (nullable = true)\n"," |-- app_id: string (nullable = true)\n"," |-- app_domain: string (nullable = true)\n"," |-- app_category: string (nullable = true)\n"," |-- device_id: string (nullable = true)\n"," |-- device_ip: string (nullable = true)\n"," |-- device_model: string (nullable = true)\n"," |-- device_type: integer (nullable = true)\n"," |-- device_conn_type: integer (nullable = true)\n"," |-- C14: integer (nullable = true)\n"," |-- C15: integer (nullable = true)\n"," |-- C16: integer (nullable = true)\n"," |-- C17: integer (nullable = true)\n"," |-- C18: integer (nullable = true)\n"," |-- C19: integer (nullable = true)\n"," |-- C20: integer (nullable = true)\n"," |-- C21: integer (nullable = true)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Fu3zD3ecqyIZ"},"source":["# Save data in parquet"]},{"cell_type":"markdown","metadata":{"id":"Apj3Hhck-17w"},"source":["Parquet has some advantages over csv, primarily ([source](https://www.youtube.com/results?search_query=spark++Parquet)):\n","- defined schemas\n","- predicate push down, e.g., filtering before processing"]},{"cell_type":"code","metadata":{"id":"aCdpCqNslPcb"},"source":["'''\n","df.coalesce(4) #Returns a new DataFrame that has exactly numPartitions partitions. \n","  .write #write a DataFrame to external storage systems (e.g. file systems, key-value stores, etc). Use DataFrame.write to access this.\n","  .mode(\"overwrite\") #Specifies the behavior when data or table already exists.\n","  .parquet(\"parquet/filtered_train.csv\")# Parquet storage formats is more efficient/fast with big data\n","# speed up sql querel by overcoming two major bottlenecks of distributed analytics: communication costs (IO bound) and data decoding (CPU bound).\n","'''\n","df.coalesce(4).write.mode('overwrite').parquet(\"/content/drive/MyDrive/LAI_OFFER/BIG_DATA/AvazuCTR/Data/parquet/train.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aHMXzsX0-1TA"},"source":["coalesce is like repartition with minimal shuffling and data moval "]},{"cell_type":"markdown","metadata":{"id":"MDETvdjX7n8n"},"source":["Complementary reading:\n","- coalesce is like repartition with minimal shuffling and data moval \n","- Spark can run 1 concurrent task for every partition of an RDD (up to the number of cores in the cluster). If you're cluster has 20 cores, you should have at least 20 partitions (in practice 2–3x times more ) [ref](https://www.google.com/search?q=spark+number+of+partitions&oq=spark+number+of+partitions&aqs=chrome..69i57j0i7i30l7.9744j0j7&sourceid=chrome&ie=UTF-8)\n","- [partition-worker node,   excutor-task](http://beginnershadoop.com/2019/09/30/distribution-of-executors-cores-and-memory-for-a-spark-application/)\n","- [stackoverflow](https://stackoverflow.com/questions/24696777/what-is-the-relationship-between-workers-worker-instances-and-executors#:~:text=A%20node%20is%20a%20machine,many%20executors%2C%20for%20many%20applications.)\n","\n","- refer to class 59\n","- excutor: a single JVM INSTANCE(/process?) ON A NODETHAT SERVERS A SINGLE SPARK application. An executor runs multiple tasks over its lifetime, and multiple tasks concurrently (?). A node may have several Spark executors and there are many nodes running Spark Executors for each client application.\n","- task: spark task represents a unit of work on a partition of a distributed dataset (RDD). \n","\n","An executor is a single JVM process which is launched for an application on a worker node. excutors are for running tasks, A single node can run multiple executors and executors for an application can span multiple worker nodes. An executor stays up for the duration of the Spark Application and runs the tasks in multiple threads. \n","- core: A core is a basic computation unit of CPU and a CPU may have one or more cores to perform tasks at a given time. The more cores we have, the more work we can do. In spark, this controls the number of parallel tasks an executor can run. Liu: It seems cores can be shared by many excutors in the same single worker node.\n","- Workers hold many executors, for many applications. One application could have executors on many workers. In Colab, only a single application is lauched."]}]}